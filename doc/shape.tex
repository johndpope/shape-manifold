\documentclass[anon,11pt]{9520} % Anonymized submission
% \documentclass{colt2012} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\newcommand{\mb}{\mathbf}

\usepackage{comment}
\usepackage{graphicx}

\title[Semi-Supervised Shape Classification]{Semi-Supervised Shape Classification with Manifold Regularization}

 % Use \Name{Author Name} to specify the name.  If the surname contains spaces,
 % enclose the surname in braces, e.g. \Name{John {Smith Jones}} similarly if
 % the name has a "von" part, e.g \Name{Jane {de Winter}}.  If the first letter
 % in the forenames is a diacritic enclose the diacritic in braces,
 % e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address \coltauthor{\Name{Author Name1}
  % \Email{abc@sample.com}\and \Name{Author Name2}
  % \Email{xyz@sample.com}\\ \addr Address}

 % Three or more authors with the same address: \coltauthor{\Name{Author Name1}
 % \Email{an1@sample.com}\\ \Name{Author Name2}
 % \Email{an2@sample.com}\\ \Name{Author Name3} \Email{an3@sample.com}\\ \addr
 % Address}


 % Authors with different addresses: 

\author{\Name{Stanislav Nikolov}
\Email{snikolov@mit.edu}\\}

\begin{document}

\maketitle

%TODO: 
% INTRO-ISH
% [x] Talk about why shapes spaces have interesting geometric structure to be exploited.
% * Better explain the PCA thumbnail plot.
% * Cite Belkin, and all those guys

% DATA
% * Cite some people for signed distance functions.

% EXPERIMENT
% [x] Talk about why we use a single sigma for the kernel.
% [x] Maybe talk about why we chose the search ranges for lambdaA and lambdaI

% RESULTS
% * Examples of classification (in PCA space).

\begin{abstract}
We approach the problem of semi-supervised shape classification by exploiting
the geometric structure of shape data. We apply {\em manifold regularization} to
learn a function from shapes to class labels. Central to manifold regularization
algorithms is the use of a weighted graph to represent pairwise relationships
between training points and capture their geometric structure. Under a
regularized least squares formulation, each algorithm only involves solving a
linear system of equations. We analyze the classification performance for
different features regularization parameters, percentage of labeled data, and
noise in the labels.  We show that encouraging the smoothness of the
classification function on the manifold improves classification performance
beyond simply encouraging smoothness in the ambient space. We compare raw pixel
features to Signed Distance Function (SDF) features and find that SDF features
are yield consistently higher accuracy. Finally, we compare manifold
regularization to the much simpler $k$-Nearest-Neighbors and show that manifold
regularization is consistently better.
\end{abstract}

\begin{keywords}
shape classification, manifold regularization, semi-supervised learning.
\end{keywords}

\section{Introduction}

% Shape classification is important
In many object recognition and classification tasks, the shape of an object is
more important than its other qualities, such as color or texture. For example
consider the task of classifying images of bottles and cups. The bottles and
cups themselves might have a variety of colors and textures, in addition to
noise and variation in background and lighting. In comparison, the silhouettes
of cups and bottles have fewer, but more important degrees of freedom. Shape
classification aims to exploit this. 

Shape classification has many applications. Crowdsourced image annotation tools
such as LabelMe \cite{LabelMe} have generated hundreds of thousands of labeled
shapes from segmentions of everyday scenes, making it as feasible as ever to
learn models for everyday shapes and exploit them to do object recognition in
complex scenes. In biomedical imaging, better classification of segmented shapes
could help with early detection of disease.

% Manifold
Recently, geometric frameworks for learning such as manifold learning and
manifold regularization (\cite{Belkin1}, \cite{Belkin2}, \cite{Belkin3}) have become a
popular approach to image and shape classification
(\cite{sdf}, \cite{Tuzel}, \cite{Zhu}, \cite{Gong}). It is well known that the space of
shapes has the structure of a low-dimensional manifold (see for example
\cite{Haykin}, Chapter 1. The idea is that the raw pixels of an image form a
feature vector in a high-dimensional space, but around each feature vector, the
possible variation in shape happens only in a small number of
directions. Roughly, the low dimension of the manifold corresponds to a low
number of latent degrees of freedom in the set of shapes.

\begin{comment}
\begin{figure}
\begin{center}
\includegraphics[width=3in]{fig/img_overlay}
\end{center}
\end{figure}
\end{comment}

\section{Manifold Regularization}
In semi-supervised learning, there is a set of $n$ points $\{\mb
x_i\}_{i=1,\dots,n}$, of which the first $\ell$ have labels
$\{y_i\}_{i=1,\dots,\ell}$ and the remaining $u=n-\ell$ are unlabeled. The goal
is to learn a function $f$ that predicts the label of an arbitrary (potentially
unobserved) point. In a fully-supervised setting, one would use only the labeled
points to learn $f$. Regularized Least Squares (RLS), for example, attempts to
learn $f$ by minimizing the regularized squared loss
\[\frac{1}{\ell}\sum_{i=1}^{\ell} (f(\mb{x}_i)-y_i)^2 + \lambda_A
\|f\|_{\mathcal{H}}^2\] which can be written as \[ \frac{1}{\ell}\|\mb{K}\mb{c}
- \mb{y}\|_2^2 + \lambda_A \mb{c}^T \mb{K} \mb{c} \] using the kernel matrix
$\mb{K}$ and coefficients $\mb{c}$ given by the Representer Theorem. Its
solution is given by taking the derivative of the objective with respect to $\mb
c$ and setting it to zero. This yields
\begin{gather}
-\frac{\mb y}{\ell} + \left( \frac{\mb K}{\ell} + \lambda_A \mb I\right)\mb c^* = 0\\
\mb c^* = \left(\mb K + \ell \lambda_A \mb I\right)^{-1} \mb y \label{eqn:crls}
\end{gather}

Manifold regularization uses the unlabeled points in addition to the labeled points in
order to exploit the geometry of the data (\cite{Belkin1}, \cite{Belkin2}, \cite{Belkin3}). Namely, it enforces the notion
that $f$ should not vary drastically in regions of high density. This idea
is captured formally by defining an intrinsic smoothness penalty on
$f$ \[\|f\|^2_{I} = \int_{\mathcal{M}} \|\nabla_{\mathcal{M}} f(x)\|^2 dp(\mb x)
= \int\Delta_{\mathcal{M}}f dp(\mb x), \] 
where $\nabla_{\mathcal{M}}$ and $\Delta_{\mathcal{M}}$ are the gradient and Laplacian of $f$ on the manifold,
respectively. This gives rise to Laplacian Regularized Least Squares (LapRLS)
where one minimizes 
\[ \frac{1}{\ell} \sum_{i=1}^{\ell} (f(\mb{x}_i)-y_i)^2 + \lambda_A \|f\|_{\mathcal{H}}^2 + \lambda_I\|f\|_I^2.\]
Furthermore, one can approximate the intrinsic smoothness penalty using only the
observed points by using a graph Laplacian instead of the manifold Laplacian:
$\mb x_1, \dots, \mb x_n$:
\[\|f\|^2_I \approx \frac{1}{2n^2} \sum_{i=1}^{n} \sum_{j=1}^{n} W_{i,j}(f_i-f_j)^2 = \mb f^T \mb L \mb f.\]
The weights
\[W_{i,j} = e^{-\frac{\|\mb{x}_i-\mb{x}_j\|^2}{2\sigma^2}}\]
are similarity weights for some scale parameter $\sigma$ and $\mb L$ is the
graph Laplacian of the graph defined by the $W_{i,j}$. Manifold regularization
has its own representer theorem, which allows us to write the objective as
\[ \frac{1}{\ell} \|\mb J(\mb K \mb c-\mb y')\|^2 + \lambda_A \mb c^T \mb K \mb c + \frac{\lambda_I}{n^2} \mb c^T \mb K \mb L \mb K \mb c \]
where $\mb{J}$ is $n$ by $n$ with ones in the first $\ell$ positions along the
diagonal and zeros everywhere else and $\mb y'$ is a vector of length $n$ with
the labels of the labeled points in the first $\ell$ values and zeros everywhere
else. Taking the derivative of the objective with respect to $\mb c$ and setting it
to zero, we get
\[ \frac{1}{\ell} \mb K \mb J \left(\mb y' - \mb J \mb K \mb c^* \right) + \left( \lambda_A \mb K  + \frac{\lambda_I \ell}{n^2} \mb K \mb L \mb K \right) \mb c^* = 0 \]
Solving for the optimal coefficients $\mb c^*$ yields
\begin{gather}
\mb c^* = \left( \mb J \mb K + \lambda_A \ell \mb I + \frac{\lambda_I\ell^2}{n^2} \mb L \mb K \right)^{-1} \mb y'. \label{eqn:claprls}
\end{gather}
To minimize the regularized loss, we simply have to solve a linear system.

\begin{comment}
\subsection{Class Imbalance}
Often, datasets do not have an equal number of positive and negative
examples. This situation, known as {\em class imbalance} makes the dominant
class more likely to be predicted, as examples from the dominant examples stand
to accumulate greater loss because of their greater number. This is typically
resolved by assigning a weight $\gamma_i$ to each point $\mb x_i$, which in our
case leads to a weighted LapRLS objective
\[ \frac{1}{\ell} \|\mb J \mb \Gamma(\mb K \mb c-\mb y')\|^2 + \lambda_A \mb c^T \mb K \mb c + \frac{1}{n^2} \mb c^T \mb K \mb L \mb K \mb c,\] where $\mb \Gamma$ is a diagonal matrix with weights $\gamma_1,\dots,\gamma_{\ell}$ as its first $\ell$ entries and zeros as its remaining $u$ entries. See \cite{Weiss} (G. Weiss. Mining with rarity: A unifying framework.SIGKDD Explorations, 6(1):7-19) and \cite{Kotsiantis} (Handling imbalanced datasets: A review Sotiris Kotsiantis, Dimitris Kanellopoulos, Panayiotis Pintelas)
2004.
\end{comment}

\section{Data}
To represent shapes, we used binary ``silhouette'' images from the Large Binary
Image Database \texttt{(http://www.lems.brown.edu/\textasciitilde{}dmc/)}. We randomly sampled a roughly
equal amount of images of butterflies (161), dogs (154), heads (149), and fish
(156).
\begin{figure}[h!]
\begin{center}
\includegraphics[height=1.25in]{fig/example_shapes}
\end{center}
\caption{\label{fig:image_examples} Dog, butterfly, fish and human head shapes
  as binary silhouette images.}
\end{figure}
The images are not all the same dimensions and are typically rectangular. Therefore,
via padding and cropping, we converted each image to be $s$ by $s$ pixels, where
$s$ is the larger of the maximum width and maximum height over all images.  As
an optional step, one can rescale each image so that that shapes are of
comparable size. We found most of the shapes to be of similar scale and
therefore omitted this step. Next, we created two feature representations from
each image, each of which is ``image-like'' and has the same dimensions as the
original image. The first feature representation is simply the raw pixels of the
image. The second feature representation is the discretized {\em signed distance
  function}, which we will describe shortly. Finally, to reduce
dimensionality, each feature representation is scaled down.

\subsection{Signed Distance Functions} 
A {\em signed distance function} (SDF) is an object that implicitly defines a
partition of a space into (possibly multiply-connected) inside and outside
regions. Signed distance functions are widely used in computer vision to
represent shapes (see for example \cite{sdf}, \cite{Tsai1}, \cite{Tsai2}). In our case, the space is a plane,
and the boundary between the inside and outside regions can be thought of as a
shape. SDFs take in a point $\mb{x}$ (e.g. in the plane) and give the {\em
  signed distance} to the closest point on the boundary. If $\mb{x}$ is in the
inside region, the signed distance is negative; if it is in the outside region,
the signed distance is positive. Hence, the boundary, or shape, is given by the
zero contour of the SDF.
\begin{figure}[h!]
\begin{center}
\includegraphics[width=4in]{fig/bin_vs_sdf}
\end{center}
\caption{\label{fig:bin_vs_sdf} Signed Distance Functions are implicit representations of shapes.}
\end{figure}

\section{Experiment}
\subsection{Goals}
\label{sec:goals}
Our goal is to study three things: the effect of having an intrinsic
regularization parameter $\lambda_I$, the effect of using SDF features over raw
image features, and how the performance of manifold regularization (MR) compares
to that of $k$-Nearest-Neighbors ($k$NN).

\subsection{Training/Test Split}
For each pair of datasets, we form a test set using 25\% of the data and use the
remaining 75\% to train a classifier. We reveal the labels of a fraction
$p_{labeled}$ of the training set and leave the rest unlabeled. Of those
labeled, we artificially add noise by flipping each label with probability
$p_{flip}$. We then calculate the percent correct on the test set. Figure \ref{fig:pca} illustrates this setup visually.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=3in]{fig/laprls}
\end{center}
\caption{\label{fig:pca} An illustration of classification, showing unlabeled data, labeled
  data, and classified test data. To visualize the high dimensional features, we
  plotted the data along its first two principal component.}
\end{figure} 

\subsection{Cross-Validation and Parameter Tuning}
During training, certain parameters are automatically tuned for each algorithm
by minimizing the $k$-fold cross validation error ($k$=8). We use only the
labeled points in the test portion of each validation split. That is, the points
originally chosen to be unlabeled are still considered unlabeled and are not
used for computing cross-validation error. For manifold regularization, the
parameters to be tuned are $\lambda_A$ for RLS, $\lambda_A$ and $\lambda_I$ for
LapRLS, and $\sigma$ for both RLS and LapRLS. For $k$NN, the parameter to be
tuned is simply $k$.

The range for each parameter to be tuned was chosen by trial and error on the
datasets involved. In particular, we made sure that the optimal value of the
parameter did not often lie at either end of the range searched. For the $k$ in
$k$NN we search from $k = 1, \dots, 10$. For some parameters, especially low
$p_{labeled}$, the number of labeled points in the training part of a validation
split does not allow for all values of $k$ in this range. For example, if $k =
10$ and the number of labeled points is less than 10, there cannot be 10 nearest
neighbors. In this case, we search only through the allowed values of $k$. For
$\sigma$, we compute the average distance between points and search a range of
multiples from $0.5$ to $1.5$ of this value. The average distance defines the
typical length scale to be used when computing graph weights between points. For
the regularization parameters $\lambda_A$ and $\lambda_I$, we search linearly
from 0 to $\lambda_{\max}(\mb{K})$. Roughly speaking, we would like to avoid an
ill-conditioned system in Eqs. \ref{eqn:claprls} and \ref{eqn:crls}, and
regularization parameters on the order of magnitude of the maximum eigenvalue of
$\mb K$ make this possible.

For the kernel $\mb K$, we chose the same form as the weight matrix $\mb W$
with $\sigma$ equal to a multiplicative factor of 1 times the average distance
between all points. For simplicity, we fixed the multiplicative factor to 1
after initial trial and error, and focus instead on the effects of the other
parameters.

\subsection{Multiple Trials}
The test set performance is computed for 5 different trials, with the random
seed being reset before each set of trials. This way, the same random choices of
points are involved, and we can better gauge the effect of just changes in
parameters.

\subsection{Mild and Harsh Conditions}
Each algorithm is evaluated under ``harsh'' conditions and ``mild'' conditions. Under mild conditions
 ($p_{labeled} = 0.25$, $p_{flip} = 0$), 25\% of the training set is labeled
and the labels are noiseless. Under harsh conditions ($p_{labeled} = 0.05$, $p_{flip} =
0.2$), only 5\% of the training set is labeled and the labels are noisy.

\subsection{Algorithm Comparison}
To study each effect mentioned in section \ref{sec:goals}, we propose the
following methodology.

To compare $k$NN and MR, we compare the results of $k$NN
to the best results from MR over all choices of regularization mode (RLS
vs. LapRLS) and all choices of features.

To compare raw image features and SDF features, we compare, for each pair of
datasets, the best performing MR algorithm (either RLS or LapRLS) that uses image
features, and the best-performing MR algorithm that uses SDF features.

Finally, to compare RLS and LapRLS, we directly compare their accuracy for each pair of
datasets. Note that each of these comparisons is done for each pair of datasets under both
mild and harsh conditions.

%\clearpage
\section{Results}
\subsection{Effect of Regularization}
Table \ref{tbl:rls_laprls} shows that LapRLS consistently has higher accuracy than RLS.
\begin{table}[h!]
\tiny
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{$p_{flip} = 0.2$, $p_{labeled} = 0.05$}\\
\multicolumn{5}{c}{Image features}\\
\hline
Class 1 & Class 2 & Features & Mode & Accuracy\\\hline

butterflies&	fish&	image&	RLS&	0.58$\pm$0.07\\
butterflies&	fish&	image&	LapRLS& \textbf{0.82$\pm$0.04}\\\hline

butterflies&	heads&	image&  RLS& 0.80$\pm$0.22\\
butterflies&	heads&	image&	LapRLS& \textbf{0.81$\pm$0.23}\\\hline

butterflies&	dogs&	image&	RLS&	0.62$\pm$0.17\\
butterflies&	dogs&	image&	RLS&      \textbf{0.76$\pm$0.10}\\\hline

fish&	heads&	image&	RLS&	0.75$\pm$0.16\\
fish&	heads&	image&	LapRLS&	\textbf{0.95$\pm$0.03}\\\hline

fish&	dogs&	image&	RLS&	0.61$\pm$0.14\\
fish&	dogs&	image&	LapRLS&	\textbf{0.69$\pm$0.16}\\\hline

heads&	dogs&	image&	RLS&	0.58$\pm$0.14\\
heads&	dogs&	image&	LapRLS&	\textbf{0.75$\pm$0.12}\\\hline
\multicolumn{5}{c}{SDF features}\\\hline
butterflies&	fish&	SDF&    RLS&	0.77$\pm$0.03\\
butterflies&	fish&	SDF&	LapRLS&	\textbf{0.86$\pm$0.10}\\\hline

butterflies&	heads&	SDF&	RLS&	0.98$\pm$0.03\\
butterflies&	heads&	SDF&	LapRLS&	\textbf{0.99$\pm$0.00}\\\hline

butterflies&	dogs&	SDF&	RLS&	\textbf{0.91$\pm$0.04}\\
butterflies&	dogs&	SDF&	LapRLS&	0.91$\pm$0.06\\\hline

fish&	heads&	SDF&	RLS&	0.97$\pm$0.01\\
fish&	heads&	SDF&	LapRLS&	\textbf{0.98$\pm$0.01}\\\hline

fish&	dogs&	SDF&	RLS&	0.87$\pm$0.02\\
fish&	dogs&	SDF&	LapRLS&	\textbf{0.89$\pm$0.03}\\\hline

heads&	dogs&	SDF&	RLS&	0.73$\pm$0.22\\
heads&	dogs&	SDF&	LapRLS&	\textbf{0.93$\pm$0.03}\\\hline
\end{tabular}
\begin{tabular}{|c|c|c|c|c|}
\multicolumn{5}{c}{$p_{flip} = 0$, $p_{labeled} = 0.25$}\\
\multicolumn{5}{c}{Image features}\\
\hline
Class 1 & Class 2 & Features & Mode & Accuracy\\\hline

butterflies&	fish&	image&	RLS&	0.77$\pm$0.09\\
butterflies&	fish&	image&	LapRLS&	\textbf{0.90$\pm$0.02}\\\hline

butterflies&	heads&	image&		RLS&	\textbf{0.99$\pm$0.01}\\
butterflies&	heads&	image&		LapRLS&	0.99$\pm$0.02\\\hline

butterflies&	dogs&	image&		RLS&	0.87$\pm$0.10\\
butterflies&	dogs&	image&		LapRLS&	\textbf{0.95$\pm$0.01}\\\hline

fish&	heads&	image&		RLS&			0.95$\pm$0.05\\
fish&	heads&	image&		LapRLS&			\textbf{0.99$\pm$0.00}\\\hline

fish&	dogs&	image&		RLS&			0.83$\pm$0.02\\
fish&	dogs&	image&		LapRLS&			\textbf{0.90$\pm$0.10}\\\hline

heads&	dogs&	image&		RLS&			0.85$\pm$0.12\\
heads&	dogs&	image&		LapRLS&			\textbf{0.94$\pm$0.04}\\\hline
\multicolumn{5}{c}{SDF features}\\\hline
butterflies&	fish&	SDF&		RLS&			0.93$\pm$0.01\\
butterflies&	fish&	SDF&		LapRLS&			\textbf{0.94$\pm$0.02}\\\hline

butterflies&	heads&	SDF&		RLS&			\textbf{1.00$\pm$0.00}\\
butterflies&	heads&	SDF&		LapRLS&			\textbf{1.00$\pm$0.00}\\\hline

butterflies&	dogs&	SDF&		RLS&			0.98$\pm$0.01\\
butterflies&	dogs&	SDF&		LapRLS&			\textbf{0.98$\pm$0.00}\\\hline

fish&	heads&	SDF&		RLS&			0.98$\pm$0.01\\
fish&	heads&	SDF&		LapRLS&			\textbf{0.99$\pm$0.01}\\\hline

fish&	dogs&	SDF&		RLS&			0.96$\pm$0.01\\
fish&	dogs&	SDF&		LapRLS&			\textbf{0.97$\pm$0.01}\\\hline

heads&	dogs&	SDF&		RLS&			0.94$\pm$0.01\\
heads&	dogs&	SDF&		LapRLS&			\textbf{0.99$\pm$0.01}\\\hline
\end{tabular}
\end{center}
\caption{\label{tbl:rls_laprls} LapRLS outperforms RLS for both image and SDF
  features, as well as under both noisy and sparsely labeled conditions
  ($p_{flip} = 0.2$, $p_{labeled} = 0.05$) and noiseless and more densely
  labeled conditions ($p_{flip} = 0$, $p_{labeled} = 0.25$).}
\end{table}

%\clearpage
\subsection{Comparison of Features for Manifold Regularization}
We took the better of each pair of LapRLS and RLS results from Table
\ref{tbl:rls_laprls} and compared the classification performance of raw image
feature and SDF features. Table \ref{tbl:features} shows that SDF features do
better than image features.
\begin{table}[h!]
\tiny
\begin{center}
\begin{tabular}{|c|c|c|c|}
\multicolumn{4}{c}{$p_{flip} = 0.2$, $p_{labeled} = 0.05$}\\
\hline
Class 1 & Class 2 & Features & Accuracy\\\hline

butterflies&	fish&	image&	0.82$\pm$0.04\\
butterflies&	fish&	SDF&	\textbf{0.86$\pm$0.10}\\\hline

butterflies&	heads&	image&	0.81$\pm$0.23\\
butterflies&	heads&	SDF&	\textbf{0.99$\pm$0.00}\\\hline

butterflies&	dogs&	image&	0.76$\pm$0.10\\
butterflies&	dogs&	SDF&	\textbf{0.91$\pm$0.04}\\\hline

fish&	heads&	image&	0.95$\pm$0.03\\
fish&	heads&	SDF&	\textbf{0.98$\pm$0.01}\\\hline

fish&	dogs&	image&	0.69$\pm$0.16\\
fish&	dogs&	SDF&	\textbf{0.89$\pm$0.03}\\\hline

heads&	dogs&	image&	0.75$\pm$0.12\\
heads&	dogs&	SDF&	\textbf{0.93$\pm$0.03}\\\hline
\end{tabular}
\begin{tabular}{|c|c|c|c|}
\multicolumn{4}{c}{$p_{flip} = 0$, $p_{labeled} = 0.25$}\\
\hline
Class 1 & Class 2 & Features & Accuracy\\\hline

butterflies&	fish&	image&		0.90$\pm$0.02\\
butterflies&	fish&	SDF&					\textbf{0.94$\pm$0.02}\\\hline

butterflies&	heads&	image&	0.99$\pm$0.01\\
butterflies&	heads&	SDF&					\textbf{1.00$\pm$0.00}\\\hline

butterflies&	dogs&	image&			0.95$\pm$0.01\\
butterflies&	dogs&	SDF&					\textbf{0.98$\pm$0.00}\\\hline

fish&	heads&	image&					\textbf{0.99$\pm$0.00}\\
fish&	heads&	SDF&					0.99$\pm$0.01\\\hline

fish&	dogs&	image&					0.90$\pm$0.10\\
fish&	dogs&	SDF&					\textbf{0.97$\pm$0.01}\\\hline

heads&	dogs&	image&					0.94$\pm$0.04\\
heads&	dogs&	SDF&					\textbf{0.99$\pm$0.01}\\\hline

\end{tabular}

\end{center}
\caption{\label{tbl:features} The table above compares the best results for
  image features to the best results for SDFs for manifold regularization. The
  best result for a given pair of datasets and a choice of features is the
  maximum of the accuracies given by RLS and LapRLS (found in Table
  \ref{tbl:rls_laprls}). It is clear that SDF features outperform image
  features.}
\end{table}

%\clearpage
\subsection{Comparison of Features for $k$NN}
In order to compare $k$NN to manifold regularization, we need to take the better
of each pair of SDF and raw image $k$NN results. Table \ref{tbl:knn_features}
shows the accuracy of $k$NN for each type of feature. It appears that neither
feature is clearly better than the other.
\begin{table}
\tiny
\begin{center}
\begin{tabular}{|c|c|c|c|}
\multicolumn{4}{c}{$p_{flip} = 0.2$, $p_{labeled} = 0.05$}\\
\hline
Class 1 & Class 2 & Features & Accuracy\\
\hline
butterflies&	fish&	image&	0.68$\pm$0.11\\
butterflies&	fish&	SDF&	\textbf{0.69$\pm$0.12}\\\hline

butterflies&	heads&	image&	0.97$\pm$0.04\\
butterflies&	heads&	SDF&	\textbf{0.97$\pm$0.01}\\\hline

butterflies&	dogs&	image&	\textbf{0.73$\pm$0.16}\\
butterflies&	dogs&	SDF&	0.71$\pm$0.12\\\hline

fish&	heads&	image&		\textbf{0.73$\pm$0.40}\\
fish&	heads&	SDF&		0.71$\pm$0.40\\\hline

fish&	dogs&	image&		\textbf{0.78$\pm$0.23}\\
fish&	dogs&	SDF&		0.73$\pm$0.25\\\hline

heads&	dogs&	image&		\textbf{0.75$\pm$0.22}\\
heads&	dogs&	SDF&		0.74$\pm$0.21\\\hline
\end{tabular}
\begin{tabular}{|c|c|c|c|}
\multicolumn{4}{c}{$p_{flip} = 0$, $p_{labeled} = 0.25$}\\
\hline
Class 1 & Class 2 & Features & Accuracy\\
\hline
butterflies&	fish&	image&	0.91$\pm$0.04\\
butterflies&	fish&	SDF&	\textbf{0.93$\pm$0.04}\\\hline

butterflies&	heads&	image&	\textbf{1.00$\pm$0.01}\\
butterflies&	heads&	SDF&	\textbf{1.00$\pm$0.01}\\\hline

butterflies&	dogs&	image&	\textbf{0.97$\pm$0.02}\\
butterflies&	dogs&	SDF&	\textbf{0.97$\pm$0.02}\\\hline

fish&		heads&	image&	\textbf{0.97$\pm$0.01}\\
fish&		heads&	SDF&	0.96$\pm$0.02\\\hline

fish&		dogs&	image&	\textbf{0.95$\pm$0.02}\\
fish&		dogs&	SDF&	0.94$\pm$0.03\\\hline

heads&		dogs&	image&	\textbf{0.97$\pm$0.01}\\
heads&		dogs&	SDF&	0.97$\pm$0.02\\\hline
\end{tabular}
\end{center}
\caption{\label{tbl:knn_features} The table above shows the accuracy of $k$NN for
  image features and SDF features. Neither feature choice is clearly better than
  the other.}
\end{table}

\subsection{Comparison of Manifold Regularization and $k$NN}
We compared the best results from manifold regularization to the best results
from $k$NN. We took the better of each pair of image and SDF feature results
from Table \ref{tbl:features} for manifold regularization, and from Table
\ref{tbl:knn_features} for $k$NN. Table \ref{tbl:knn_vs_mr} shows that manifold
regulariation has higher accuracy.
\begin{table}
\tiny
\begin{center}
\begin{tabular}{|c|c|c|c|}
\multicolumn{4}{c}{$p_{flip} = 0.2$, $p_{labeled} = 0.05$}\\
\hline
Class 1 & Class 2 & Method & Accuracy\\
\hline
%knn
butterflies&	fish&  $k$NN&  0.69$\pm$0.12\\
butterflies&	fish&	MR& \textbf{0.86$\pm$0.10}\\\hline

butterflies&	heads&	$k$NN& 0.97$\pm$0.01\\
butterflies&	heads&	MR& \textbf{0.99$\pm$0.00}\\\hline

butterflies&	dogs&	$k$NN& 0.73$\pm$0.16\\
butterflies&	dogs&	MR& \textbf{0.91$\pm$0.04}\\\hline

fish&	heads&		$k$NN& 0.73$\pm$0.40\\
fish&	heads&	MR& \textbf{0.98$\pm$0.01}\\\hline

fish&	dogs&		$k$NN& 0.78$\pm$0.23\\
fish&	dogs&	MR& \textbf{0.89$\pm$0.03}\\\hline

heads&	dogs&	$k$NN&	0.75$\pm$0.22\\
heads&	dogs&	MR& \textbf{0.93$\pm$0.03}\\\hline
\end{tabular}
\begin{tabular}{|c|c|c|c|}
\multicolumn{4}{c}{$p_{flip} = 0$, $p_{labeled} = 0.25$}\\
\hline
Class 1 & Class 2 & Method & Accuracy\\
\hline
butterflies&	fish&	$k$NN& 0.93$\pm$0.04\\
butterflies&	fish&	MR& 	\textbf{0.94$\pm$0.02}\\\hline

butterflies&	heads&	$k$NN& 1.00$\pm$0.01\\
butterflies&	heads&		MR& 			\textbf{1.00$\pm$0.00}\\

butterflies&	dogs&	$k$NN& 0.97$\pm$0.02\\
butterflies&	dogs&			MR& 		\textbf{0.98$\pm$0.00}\\\hline

fish&		heads&	$k$NN& 0.97$\pm$0.01\\
fish&	heads&		MR& 			\textbf{0.99$\pm$0.00}\\\hline

fish&		dogs&	$k$NN& 0.95$\pm$0.02\\
fish&	dogs&			MR& 		\textbf{0.97$\pm$0.01}\\\hline

heads&		dogs&	$k$NN& 0.97$\pm$0.01\\
heads&	dogs&		MR& 			\textbf{0.99$\pm$0.01}\\\hline
\end{tabular}
\end{center}
\caption{\label{tbl:knn_vs_mr} A comparison of the best-performing versions of
  $k$-Nearest-Neighbors and Manifold Regularization. Manifold Regularization
  clearly outperforms $k$NN.}
\end{table}

\section{Discussion and Summary}
\subsection{LapRLS vs. RLS}
It is no surprise that LapRLS has better classification performance than
RLS. Spaces of shapes are well-known to have a low-dimensional manifold
structure, and only LapRLS is able to exploit it. For sufficiently mild
conditions and when using the better-performing SDF features, accuracy is often
near-perfect, and the classification performances of LapRLS and RLS become
comparable. On real world data sets, where near perfect accuracy is more
difficult to achieve, we expect to see a greater performance difference.

\subsection{Manifold Regularization vs. $k$NN}
While LapRLS exploits exploits the geometry of the training data by making use
ofs the unlabeled data, $k$NN does not use unlabeled data at all. Under harsh
conditions, MR greatly outperforms $k$NN for all datasets except ``butterflies''
vs. ``heads'' where it only outperforms $k$NN by a small margin. Under mild
conditions, however, the performance of $k$NN is comparable to that of MR, and
it might not be worth implementing MR methods. However, the datasets used are
artificial and the performance differences on real world data might be more
dramatic.

\subsection{SDF Features vs. Image Features}
SDFs features outperform image features, sometimes dramatically so under harsh
conditions such as sparse or noisy labels. We believe this to be because SDFs
are smoother than raw images and better obey the manifold assumption. To get a
sense for this, consider binary images of handwritten 2s. Two 2s might be very
similar in shape, yet have almost no overlap because they are so narrow in most
places. The SDFs of those shapes, however, will overlap significantly because
information about the shape boundary is propagated across all values of the
feature vector. Thus, shapes that would be considered close are in fact close in
the feature space, leading to a more well-behaved manifold.

\bibliography{shape.bib}

\end{document}
